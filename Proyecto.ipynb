{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el dataset\n",
    "df = pd.read_csv('Tweets.csv')\n",
    "\n",
    "# Ver las primeras filas del dataset\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['textID', 'text', 'selected_text', 'sentiment'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ver las columnas del dataset\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       textID                                               text  \\\n",
      "0  cb774db0d1                I`d have responded, if I were going   \n",
      "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
      "2  088c60f138                          my boss is bullying me...   \n",
      "3  9642c003ef                     what interview! leave me alone   \n",
      "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
      "\n",
      "                         selected_text sentiment  \\\n",
      "0  I`d have responded, if I were going   neutral   \n",
      "1                             Sooo SAD  negative   \n",
      "2                          bullying me  negative   \n",
      "3                       leave me alone  negative   \n",
      "4                        Sons of ****,  negative   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0                  id have responded if i were going  \n",
      "1         sooo sad i will miss you here in san diego  \n",
      "2                             my boss is bullying me  \n",
      "3                      what interview leave me alone  \n",
      "4   sons of  why couldnt they put them on the rel...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "# Reemplazar NaN por cadenas vacías en la columna 'text'\n",
    "df['text'] = df['text'].fillna('')\n",
    "\n",
    "# Definir la función de limpieza del texto\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):  # Verifica si el valor es una cadena\n",
    "        return ''  # Devuelve una cadena vacía si no lo es\n",
    "    # Aquí puedes agregar las operaciones de limpieza (como eliminar caracteres especiales, convertir a minúsculas, etc.)\n",
    "    text = text.lower()  # Convertir el texto a minúsculas\n",
    "    text = re.sub(r'http\\S+', '', text)  # Eliminar URLs\n",
    "    text = re.sub(r'@\\w+', '', text)  # Eliminar menciones (@usuario)\n",
    "    text = re.sub(r'#\\w+', '', text)  # Eliminar hashtags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Eliminar caracteres no alfabéticos\n",
    "    return text\n",
    "\n",
    "# Aplicar la función de limpieza a la columna 'text'\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Ver las primeras filas después de limpiar\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\Documents\\David\\Proyecto_IA\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', download_dir=r\"C:\\Users\\admin\\Documents\\David\\Proyecto_IA\\nltk_data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\Documents\\David\\Proyecto_IA\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\Documents\\David\\Proyecto_IA\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\Documents\\David\\Proyecto_IA\\nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt', download_dir=r\"C:\\Users\\admin\\Documents\\David\\Proyecto_IA\\nltk_data\")\n",
    "nltk.download('stopwords', download_dir=r\"C:\\Users\\admin\\Documents\\David\\Proyecto_IA\\nltk_data\")\n",
    "nltk.download('wordnet', download_dir=r\"C:\\Users\\admin\\Documents\\David\\Proyecto_IA\\nltk_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\admin/nltk_data', 'c:\\\\Python313\\\\nltk_data', 'c:\\\\Python313\\\\share\\\\nltk_data', 'c:\\\\Python313\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\admin\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.data.path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:/Users/admin/Documents/David/Proyecto_IA/nltk_data..\n",
      "[nltk_data]     .\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "ruta_personalizada = \"C:/Users/admin/Documents/David/Proyecto_IA/nltk_data\"\n",
    "nltk.download('punkt_tab', download_dir=ruta_personalizada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK encontró: C:\\Users\\admin\\Documents\\David\\Proyecto_IA\\nltk_data\\tokenizers\\punkt\\english.pickle\n",
      "Tokens: ['I', '`', 'd', 'have', 'responded', ',', 'if', 'I', 'were', 'going']\n"
     ]
    }
   ],
   "source": [
    "nltk.data.path.clear()\n",
    "nltk.data.path.append(ruta_personalizada)\n",
    "\n",
    "# 3. Verifica que nltk \"vea\" el archivo\n",
    "try:\n",
    "    from nltk.data import find\n",
    "    print(\"NLTK encontró:\", find('tokenizers/punkt/english.pickle'))\n",
    "except LookupError:\n",
    "    print(\"No se encontró punkt. Intentando descargarlo...\")\n",
    "\n",
    "    # 4. Descargar si no se encuentra\n",
    "    nltk.download('punkt', download_dir=ruta_personalizada)\n",
    "\n",
    "\n",
    "    print(\"Reintentando...\")\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    text = \"I`d have responded, if I were going\"\n",
    "    tokens = word_tokenize(text)\n",
    "    print(\"Tokens:\", tokens)\n",
    "else:\n",
    "\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    text = \"I`d have responded, if I were going\"\n",
    "    tokens = word_tokenize(text)\n",
    "    print(\"Tokens:\", tokens)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0                I`d have responded, if I were going   \n",
      "1      Sooo SAD I will miss you here in San Diego!!!   \n",
      "2                          my boss is bullying me...   \n",
      "3                     what interview! leave me alone   \n",
      "4   Sons of ****, why couldn`t they put them on t...   \n",
      "\n",
      "                                        cleaned_text  \\\n",
      "0                  id have responded if i were going   \n",
      "1         sooo sad i will miss you here in san diego   \n",
      "2                             my boss is bullying me   \n",
      "3                      what interview leave me alone   \n",
      "4   sons of  why couldnt they put them on the rel...   \n",
      "\n",
      "                          lemmatized_text  \n",
      "0                      id responded going  \n",
      "1                 sooo sad miss san diego  \n",
      "2                            bos bullying  \n",
      "3                   interview leave alone  \n",
      "4  son couldnt put release already bought  \n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Inicializar stopwords y lematizador\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Función para procesar texto: tokenizar, quitar stopwords y lematizar\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words and word.isalpha()]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Aplicar al DataFrame\n",
    "df['lemmatized_text'] = df['cleaned_text'].apply(preprocess_text)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(df[['text', 'cleaned_text', 'lemmatized_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Crear una lista de (texto, etiqueta)\n",
    "data = list(zip(df['lemmatized_text'], df['sentiment']))\n",
    "\n",
    "# Mezclar aleatoriamente\n",
    "random.shuffle(data)\n",
    "\n",
    "# 80% entrenamiento, 20% prueba\n",
    "split_idx = int(0.8 * len(data))\n",
    "train_data = data[:split_idx]\n",
    "test_data = data[split_idx:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "# Inicializar estructuras\n",
    "class_counts = defaultdict(int)\n",
    "word_counts = defaultdict(lambda: defaultdict(int))\n",
    "vocab = set()\n",
    "\n",
    "# Entrenar\n",
    "for text, label in train_data:\n",
    "    class_counts[label] += 1\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        word_counts[label][word] += 1\n",
    "        vocab.add(word)\n",
    "\n",
    "# Calcular total de documentos\n",
    "total_docs = sum(class_counts.values())\n",
    "\n",
    "# Calcular probabilidades log(palabra|clase) con Laplace\n",
    "log_probs = {}\n",
    "class_priors = {}\n",
    "\n",
    "for label in class_counts:\n",
    "    total_words = sum(word_counts[label].values())\n",
    "    class_priors[label] = math.log(class_counts[label] / total_docs)\n",
    "    log_probs[label] = {}\n",
    "\n",
    "    for word in vocab:\n",
    "        word_freq = word_counts[label][word] + 1  # Laplace\n",
    "        log_probs[label][word] = math.log(word_freq / (total_words + len(vocab)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    words = text.split()\n",
    "    scores = {}\n",
    "\n",
    "    for label in class_priors:\n",
    "        scores[label] = class_priors[label]\n",
    "        for word in words:\n",
    "            if word in vocab:\n",
    "                scores[label] += log_probs[label].get(word, math.log(1 / (sum(word_counts[label].values()) + len(vocab))))\n",
    "    \n",
    "    return max(scores, key=scores.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo Naive Bayes: 0.64\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = len(test_data)\n",
    "\n",
    "for text, label in test_data:\n",
    "    prediction = predict(text)\n",
    "    if prediction == label:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Precisión del modelo Naive Bayes: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "neutral     11118\n",
       "positive     8582\n",
       "negative     7781\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo Naive Bayes: 0.64\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "# Lista de stopwords (puedes agregar más palabras si lo deseas)\n",
    "stopwords = set([\"the\", \"and\", \"is\", \"to\", \"it\", \"of\", \"a\", \"in\", \"that\", \"for\", \"on\", \"with\", \"as\", \"was\", \"at\", \"by\"])\n",
    "\n",
    "# Función para limpiar el texto (eliminar stopwords y convertir a minúsculas)\n",
    "def clean_text(text):\n",
    "    words = text.split()\n",
    "    return ' '.join([word.lower() for word in words if word not in stopwords])\n",
    "\n",
    "# Cargar el dataset (deberías cargar tu archivo CSV aquí)\n",
    "df = pd.read_csv('Tweets.csv')\n",
    "\n",
    "# Preprocesar los datos\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "df['lemmatized_text'] = df['cleaned_text']  # Aquí podrías aplicar lematización si lo deseas\n",
    "\n",
    "# Crear una lista de (texto, etiqueta)\n",
    "data = list(zip(df['lemmatized_text'], df['sentiment']))\n",
    "\n",
    "# Mezclar aleatoriamente\n",
    "random.shuffle(data)\n",
    "\n",
    "# 80% entrenamiento, 20% prueba\n",
    "split_idx = int(0.8 * len(data))\n",
    "train_data = data[:split_idx]\n",
    "test_data = data[split_idx:]\n",
    "\n",
    "# Inicializar estructuras\n",
    "class_counts = defaultdict(int)\n",
    "word_counts = defaultdict(lambda: defaultdict(int))\n",
    "vocab = set()\n",
    "\n",
    "# Entrenar\n",
    "for text, label in train_data:\n",
    "    class_counts[label] += 1\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        word_counts[label][word] += 1\n",
    "        vocab.add(word)\n",
    "\n",
    "# Calcular total de documentos\n",
    "total_docs = sum(class_counts.values())\n",
    "\n",
    "# Calcular probabilidades log(palabra|clase) con Laplace\n",
    "log_probs = {}\n",
    "class_priors = {}\n",
    "\n",
    "for label in class_counts:\n",
    "    total_words = sum(word_counts[label].values())\n",
    "    class_priors[label] = math.log(class_counts[label] / total_docs)\n",
    "    log_probs[label] = {}\n",
    "\n",
    "    for word in vocab:\n",
    "        word_freq = word_counts[label][word] + 1  # Laplace\n",
    "        log_probs[label][word] = math.log(word_freq / (total_words + len(vocab)))\n",
    "\n",
    "def predict(text):\n",
    "    words = text.split()\n",
    "    scores = {}\n",
    "\n",
    "    for label in class_priors:\n",
    "        scores[label] = class_priors[label]\n",
    "        for word in words:\n",
    "            if word in vocab:\n",
    "                scores[label] += log_probs[label].get(word, math.log(1 / (sum(word_counts[label].values()) + len(vocab))))\n",
    "\n",
    "        # Ajusta por el desequilibrio de clases\n",
    "        if class_counts[label] < total_docs * 0.2:  # Para clases minoritarias\n",
    "            scores[label] *= 1.5  # Aumenta el peso de las clases minoritarias\n",
    "\n",
    "    return max(scores, key=scores.get)\n",
    "\n",
    "# Evaluación del modelo\n",
    "correct = 0\n",
    "total = len(test_data)\n",
    "\n",
    "for text, label in test_data:\n",
    "    prediction = predict(text)\n",
    "    if prediction == label:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Precisión del modelo Naive Bayes: {accuracy:.2f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
